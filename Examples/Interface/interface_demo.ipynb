{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interface Type 1: Using ACT-R's default window\n",
    "    \n",
    "ACT-R comes with a default window, which can be made invisible if one wants to. Using the default window is perhaps the easiest way to implement a simple task.\n",
    "\n",
    "## The Lexical Decision task\n",
    "\n",
    "To start, we will simulate a simple _lexical decision_ task. In the task, a string of letters appears on the screen and the model has to decide whether the string forms an english word (e.g., \"zebra\") or not (e.g. \"xyzzy\").  \n",
    "    \n",
    "The model is contained in the `lexical.lisp` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACT-R connection has been started.\n"
     ]
    }
   ],
   "source": [
    "import actr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and initializing the window\n",
    "To use ACT-R's default window, a number of initialization steps need to be done before running the model. In order, these steps are:\n",
    "* Loading the model\n",
    "* Creating the window, using the predefined `open_exp_window` method.\n",
    "* Install the window device, using the predefined `install_device` method. This will effectively connect the window to ACT-R.\n",
    "* Finally, add all the necessary elements to the window. For example, to add text, one might use `add_text_to_exp_window`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    \"\"\"Sets up an ACT-R experimental window. If the function is called\n",
    "    while the ACT-R Environment is running, the window will be visible.\n",
    "    If not, the window will be automatically made 'invisible' (visible to the \n",
    "    model but not to the experimenter)\n",
    "    \"\"\"\n",
    "    actr.reset()\n",
    "    actr.load_act_r_model(\"lexical.lisp\")\n",
    "    win = actr.open_exp_window(\"Test\", width=400, height=400, visible=True)\n",
    "    actr.install_device(win)\n",
    "    actr.add_text_to_exp_window(win, \"zebra\", x=200, y=200, color=\"black\")\n",
    "    return win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the model, we simply need to call `setup()`. To see what this window looks like, we can inspect the `win` object returned by `setup`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#|Warning: No handler available for displaying a visible window.  Using a virtual window instead. |#\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vision', 'exp-window', 'Test']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win = setup()\n",
    "win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the internal representation of the windo is just _a list of three strings_. Of course, the _real_ window object is hidden; this list is simply use as a handle for the methods in the `actr.py` file to keep track of which visual objects exists and how they should be visible to ACT-R.  \n",
    "## The visicon\n",
    "At this point, we need to talk about ACT-R way of seeing things. ACT-R follows an old-fashioned model of visual attention developed by Anne Treisman. In this model, elementray visual features are immediately available to the visual module without moving attention. These features correspond to specific attributes (e.g., color, orientation, shape), and their position on the visual field. The distribution of these elements in the visual field is called the __visicon__. The visicon can be inspected an any time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.000   ------                 Stopped because event limit reached\n",
      "Name              Att  Loc             TEXT  KIND  COLOR  WIDTH  VALUE    HEIGHT  SIZE        \n",
      "----------------  ---  --------------  ----  ----  -----  -----  -------  ------  ----------\n",
      "VISUAL-LOCATION0  NEW  (519 506 1080)  T     TEXT  BLACK  35     \"zebra\"  10      0.97999996\n"
     ]
    }
   ],
   "source": [
    "actr.run_n_events(2)  # This is needed to let ACT-R's scheduler process the window first.\n",
    "actr.print_visicon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the visicon is just a list of features. These features are represented, by default, in a special chunk type called _visual-location_, each of which contains some helpful attributes such as whether the object contains text (T/NIL), its color, its location, and its size. \n",
    "\n",
    "With the visicon now in place, the model can process the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.000   VISION                 SET-BUFFER-CHUNK VISUAL-LOCATION VISUAL-LOCATION0 NIL\n",
      "     0.000   VISION                 visicon-update\n",
      "     0.000   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.050   PROCEDURAL             PRODUCTION-FIRED LOOK-AT-STRING\n",
      "     0.050   PROCEDURAL             CLEAR-BUFFER VISUAL-LOCATION\n",
      "     0.050   VISION                 Find-location\n",
      "     0.050   VISION                 SET-BUFFER-CHUNK VISUAL-LOCATION VISUAL-LOCATION0\n",
      "     0.050   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.100   VISION                 Move-attention VISUAL-LOCATION0-1\n",
      "     0.100   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.185   VISION                 Encoding-complete VISUAL-LOCATION0-1 NIL\n",
      "     0.185   VISION                 SET-BUFFER-CHUNK VISUAL TEXT0\n",
      "     0.185   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.235   PROCEDURAL             PRODUCTION-FIRED DECIDE\n",
      "     0.235   PROCEDURAL             CLEAR-BUFFER RETRIEVAL\n",
      "     0.235   DECLARATIVE            start-retrieval\n",
      "     0.235   DECLARATIVE            RETRIEVED-CHUNK ZEBRA\n",
      "     0.235   DECLARATIVE            SET-BUFFER-CHUNK RETRIEVAL ZEBRA\n",
      "     0.235   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.285   PROCEDURAL             PRODUCTION-FIRED RESPOND-WORD\n",
      "     0.285   PROCEDURAL             CLEAR-BUFFER MANUAL\n",
      "     0.285   MOTOR                  PUNCH HAND LEFT FINGER INDEX\n",
      "     0.285   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.300   ------                 Stopped because time limit reached\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3, 42, None]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actr.run(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and Attending Objects \n",
    "\n",
    "If you look at the `lexical.lisp` file, its entire visual processing is made of two productions, `look-at-string` and `decide`. The first one, `look-at-string`, finds a specific text object in the visicon by asking the `visual-location` buffer to find any object that has the property `kind text`m (i.e., it is a text). This will put the corresponding visual-location object in the `visual-location` buffer. Because we have set the `auto-attend` feature to `t`, everything that ends up in the `visual-location` buffer is also processed in the `visual` buffer.\n",
    "\n",
    "So, let's inspect the contents of this buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISUAL: TEXT0-0 [TEXT0]\n",
      "TEXT0-0\n",
      "   SCREEN-POS  VISUAL-LOCATION0-1\n",
      "   VALUE  \"zebra\"\n",
      "   COLOR  BLACK\n",
      "   HEIGHT  10\n",
      "   WIDTH  35\n",
      "   TEXT  T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['TEXT0-0']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspects the visual buffer\n",
    "actr.buffer_chunk(\"visual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interface Type 2: Defining your own visual features\n",
    "\n",
    "The experimental window is pretty limited. It can add only text and, by default, the text is transformed into special objects that only record the color and the text string (as a string, not even as chunk). It can add horizontal and vertical lines. With some effort, it can add images, but not truly an image description.\n",
    "\n",
    "What if you needed to encode much more infromation for a single word, such as font, and size? What if you wanted your visual objects to be immediately recognized as words or nonwords? That, unfortunately, cannot be done.\n",
    "\n",
    "To create complex interfaces, a programmer must be able to directly act on the visicon, generating visual locations and visual objects as they please.\n",
    "\n",
    "To do so, we will will consider a similar task. Instead of judging whether a string is a word or a nonw-word, the new model will look at a visual stimulus and decided whether the object represented in the picture is natural or man-made."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
