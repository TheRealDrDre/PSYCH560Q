{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interface Type 1: Using ACT-R's default window\n",
    "    \n",
    "ACT-R comes with a default window, which can be made invisible if one wants to. Using the default window is perhaps the easiest way to implement a simple task.\n",
    "\n",
    "## The Lexical Decision task\n",
    "\n",
    "To start, we will simulate a simple _lexical decision_ task. In the task, a string of letters appears on the screen and the model has to decide whether the string forms an english word (e.g., \"zebra\") or not (e.g. \"xyzzy\").  \n",
    "    \n",
    "The model is contained in the `lexical.lisp` file.\n",
    "\n",
    "## Starting ACT-R\n",
    "\n",
    "Before doing anything, we need to have ACT-R running in the background. Then, we can initiate the Python connection by loading the `actr.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import actr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and initializing the window\n",
    "To use ACT-R's default window, a number of initialization steps need to be done before running the model. In order, these steps are:\n",
    "* Loading the model\n",
    "* Creating the window, using the predefined `open_exp_window` method.\n",
    "* Install the window device, using the predefined `install_device` method. This will effectively connect the window to ACT-R.\n",
    "* Finally, add all the necessary elements to the window. For example, to add text, one might use `add_text_to_exp_window`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_lexical():\n",
    "    \"\"\"Sets up an ACT-R experimental window. If the function is called\n",
    "    while the ACT-R Environment is running, the window will be visible.\n",
    "    If not, the window will be automatically made 'invisible' (visible to the \n",
    "    model but not to the experimenter)\n",
    "    \"\"\"\n",
    "    actr.reset()\n",
    "    actr.load_act_r_model(\"lexical.lisp\")\n",
    "    win = actr.open_exp_window(\"Test\", width=400, height=400, visible=True)\n",
    "    actr.install_device(win)\n",
    "    actr.add_text_to_exp_window(win, \"zebra\", x=200, y=200, color=\"black\")\n",
    "    return win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the model, we simply need to call `setup()`. To see what this window looks like, we can inspect the `win` object returned by `setup`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#|Warning: No handler available for displaying a visible window.  Using a virtual window instead. |#\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vision', 'exp-window', 'Test']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win = setup_lexical()\n",
    "win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the internal representation of the windo is just _a list of three strings_. Of course, the _real_ window object is hidden; this list is simply use as a handle for the methods in the `actr.py` file to keep track of which visual objects exists and how they should be visible to ACT-R.  \n",
    "## The visicon\n",
    "At this point, we need to talk about ACT-R way of seeing things. ACT-R follows an old-fashioned model of visual attention developed by Anne Treisman. In this model, elementray visual features are immediately available to the visual module without moving attention. These features correspond to specific attributes (e.g., color, orientation, shape), and their position on the visual field. The distribution of these elements in the visual field is called the __visicon__. The visicon can be inspected an any time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.000   ------                 Stopped because event limit reached\n",
      "Name              Att  Loc             TEXT  KIND  COLOR  WIDTH  VALUE    HEIGHT  SIZE        \n",
      "----------------  ---  --------------  ----  ----  -----  -----  -------  ------  ----------\n",
      "VISUAL-LOCATION0  NEW  (519 506 1080)  T     TEXT  BLACK  35     \"zebra\"  10      0.97999996\n"
     ]
    }
   ],
   "source": [
    "actr.run_n_events(2)  # This is needed to let ACT-R's scheduler process the window first.\n",
    "actr.print_visicon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the visicon is just a list of features. These features are represented, by default, in a special chunk type called _visual-location_, each of which contains some helpful attributes such as whether the object contains text (T/NIL), its color, its location, and its size. \n",
    "\n",
    "With the visicon now in place, the model can process the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.000   VISION                 SET-BUFFER-CHUNK VISUAL-LOCATION VISUAL-LOCATION0 NIL\n",
      "     0.000   VISION                 visicon-update\n",
      "     0.000   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.050   PROCEDURAL             PRODUCTION-FIRED LOOK-AT-STRING\n",
      "     0.050   PROCEDURAL             CLEAR-BUFFER VISUAL-LOCATION\n",
      "     0.050   VISION                 Find-location\n",
      "     0.050   VISION                 SET-BUFFER-CHUNK VISUAL-LOCATION VISUAL-LOCATION0\n",
      "     0.050   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.100   VISION                 Move-attention VISUAL-LOCATION0-1\n",
      "     0.100   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.185   VISION                 Encoding-complete VISUAL-LOCATION0-1 NIL\n",
      "     0.185   VISION                 SET-BUFFER-CHUNK VISUAL TEXT0\n",
      "     0.185   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.235   PROCEDURAL             PRODUCTION-FIRED DECIDE\n",
      "     0.235   PROCEDURAL             CLEAR-BUFFER RETRIEVAL\n",
      "     0.235   DECLARATIVE            start-retrieval\n",
      "     0.235   DECLARATIVE            RETRIEVED-CHUNK ZEBRA\n",
      "     0.235   DECLARATIVE            SET-BUFFER-CHUNK RETRIEVAL ZEBRA\n",
      "     0.235   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.285   PROCEDURAL             PRODUCTION-FIRED RESPOND-WORD\n",
      "     0.285   PROCEDURAL             CLEAR-BUFFER MANUAL\n",
      "     0.285   MOTOR                  PUNCH HAND LEFT FINGER INDEX\n",
      "     0.285   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.300   ------                 Stopped because time limit reached\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3, 42, None]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actr.run(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and Attending Objects \n",
    "\n",
    "If you look at the `lexical.lisp` file, its entire visual processing is made of two productions, `look-at-string` and `decide`. The first one, `look-at-string`, finds a specific text object in the visicon by asking the `visual-location` buffer to find any object that has the property `kind text`m (i.e., it is a text). This will put the corresponding visual-location object in the `visual-location` buffer. Because we have set the `auto-attend` feature to `t`, everything that ends up in the `visual-location` buffer is also processed in the `visual` buffer.\n",
    "\n",
    "So, let's inspect the contents of these buffers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISUAL-LOCATION: VISUAL-LOCATION0-1 [VISUAL-LOCATION0]\n",
      "VISUAL-LOCATION0-1\n",
      "   KIND  TEXT\n",
      "   VALUE  TEXT\n",
      "   COLOR  BLACK\n",
      "   HEIGHT  10\n",
      "   WIDTH  35\n",
      "   SCREEN-X  519\n",
      "   SCREEN-Y  506\n",
      "   DISTANCE  1080\n",
      "   SIZE  0.97999996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['VISUAL-LOCATION0-1']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspects the visual-location buffer\n",
    "actr.buffer_chunk(\"visual-location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `visual-location` buffer contains, as expected, the visual location chunk that was in the visicon. Note that the chunk contains just minimal information: what the object is, what color it is, its size and position, and whether it is a string that looks like text. But, of course, no lexical decision can be made, because we don't know the exact text. \n",
    "\n",
    "If we look at the `visual` buffer, instead, we find a much more useful representation of the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISUAL: TEXT0-0 [TEXT0]\n",
      "TEXT0-0\n",
      "   SCREEN-POS  VISUAL-LOCATION0-1\n",
      "   VALUE  \"zebra\"\n",
      "   COLOR  BLACK\n",
      "   HEIGHT  10\n",
      "   WIDTH  35\n",
      "   TEXT  T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['TEXT0-0']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspects the *visual* buffer\n",
    "actr.buffer_chunk(\"visual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a `visual-object` chunk type. One of its values (`screen-pos`) is the visual-location itself. Some of the visual location values (as its width and height) are the same as the visual location. However, this new type of chunk does not contain information about the object's position (that is what the `visual-location` buffer is for: spatial attention). Importantly, the visual object contains some additional information, such as the fact the this text has a `value`, and it is the string \"zebra\". \n",
    "\n",
    "How did this happen? The visicon really contains a list of _paired items_. In each pair, one is the visual location chunk, which contains information about where visual features can be found in space. The second one, the visual object, contains detailed information about the object itself (e.g., the exact value of the string of text). These pairs of objects are automatically handled by ACT_R's default experimental window. \n",
    "\n",
    "When it attends a specific visual location, ACT-R just encodes the corresponding visual object in the visual buffer. \n",
    "\n",
    "# Model Interface Type 2: Defining your own visual features\n",
    "\n",
    "The experimental window is pretty limited. It can add only text and, by default, the text is transformed into special objects that only record the color and the text string (as a string, not even as chunk). It can add horizontal and vertical lines. With some effort, it can add images, but not truly an image description.\n",
    "\n",
    "What if you needed to encode much more infromation for a single word, such as font, and size? What if you wanted your visual objects to be immediately recognized as words or nonwords? That, unfortunately, cannot be done.\n",
    "\n",
    "To create complex interfaces, a programmer must be able to directly act on the visicon, generating visual locations and visual objects as they please.\n",
    "\n",
    "To do so, we will will consider a similar task. Instead of judging whether a string is a word or a nonw-word, the new model will look at a visual stimulus and decided whether the object represented in the picture is natural or man-made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['ISA', ['image-location', 'image-object'], \n",
    "            'screen-x', 200,\n",
    "            'screen-y', 200,\n",
    "            'value', ['image', 'zebra'],\n",
    "            'width', 200,\n",
    "            'height', 200,\n",
    "            'color', 'black-and-white',\n",
    "            'kind', ['image', None],\n",
    "            'content', [None, 'zebra'],\n",
    "            'style', ['photo', 'photo']\n",
    "           ]\n",
    "\n",
    "def setup_stimulus():\n",
    "    \"\"\"Sets up an ACT-R experimental window. If the function is called\n",
    "    while the ACT-R Environment is running, the window will be visible.\n",
    "    If not, the window will be automatically made 'invisible' (visible to the \n",
    "    model but not to the experimenter)\n",
    "    \"\"\"\n",
    "    actr.reset()\n",
    "    actr.load_act_r_model(\"stimulus.lisp\")\n",
    "    actr.add_visicon_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine what the visicon looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.000   ------                 Stopped because event limit reached\n",
      "Name             Att  Loc             STYLE  CONTENT  KIND   COLOR            HEIGHT  WIDTH  VALUE  SIZE        \n",
      "---------------  ---  --------------  -----  -------  -----  ---------------  ------  -----  -----  ----------\n",
      "IMAGE-LOCATION0  NEW  (200 200 1080)  PHOTO  ZEBRA    IMAGE  BLACK-AND-WHITE  200     200    ZEBRA  111.939995\n"
     ]
    }
   ],
   "source": [
    "setup_stimulus()\n",
    "actr.run_n_events(2)\n",
    "actr.print_visicon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the model until a response is made, and examine its buffers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.000   VISION                 SET-BUFFER-CHUNK VISUAL-LOCATION IMAGE-LOCATION0 NIL\n",
      "     0.000   VISION                 visicon-update\n",
      "     0.000   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.050   PROCEDURAL             PRODUCTION-FIRED FIND-PICTURE\n",
      "     0.050   PROCEDURAL             CLEAR-BUFFER VISUAL-LOCATION\n",
      "     0.050   VISION                 Find-location\n",
      "     0.050   VISION                 SET-BUFFER-CHUNK VISUAL-LOCATION IMAGE-LOCATION0\n",
      "     0.050   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.100   VISION                 Move-attention IMAGE-LOCATION0-1\n",
      "     0.100   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.185   VISION                 Encoding-complete IMAGE-LOCATION0-1 NIL\n",
      "     0.185   VISION                 SET-BUFFER-CHUNK VISUAL IMAGE-OBJECT0\n",
      "     0.185   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.235   PROCEDURAL             PRODUCTION-FIRED DECIDE\n",
      "     0.235   PROCEDURAL             CLEAR-BUFFER RETRIEVAL\n",
      "     0.235   DECLARATIVE            start-retrieval\n",
      "     0.235   DECLARATIVE            RETRIEVED-CHUNK ZEBRA-CONCEPT\n",
      "     0.235   DECLARATIVE            SET-BUFFER-CHUNK RETRIEVAL ZEBRA-CONCEPT\n",
      "     0.235   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.285   PROCEDURAL             PRODUCTION-FIRED RESPOND-NATURAL\n",
      "     0.285   PROCEDURAL             CLEAR-BUFFER RETRIEVAL\n",
      "     0.285   PROCEDURAL             CLEAR-BUFFER MANUAL\n",
      "     0.285   MOTOR                  PUNCH HAND LEFT FINGER INDEX\n",
      "     0.285   PROCEDURAL             CONFLICT-RESOLUTION\n",
      "     0.300   ------                 Stopped because time limit reached\n",
      "VISUAL: IMAGE-OBJECT0-0 [IMAGE-OBJECT0]\n",
      "IMAGE-OBJECT0-0\n",
      "   CONTENT  ZEBRA\n",
      "   STYLE  PHOTO\n",
      "   SCREEN-POS  IMAGE-LOCATION0-1\n",
      "   VALUE  ZEBRA\n",
      "   COLOR  BLACK-AND-WHITE\n",
      "   HEIGHT  200\n",
      "   WIDTH  200\n",
      "VISUAL-LOCATION: IMAGE-LOCATION0-1 [IMAGE-LOCATION0]\n",
      "IMAGE-LOCATION0-1\n",
      "   KIND  IMAGE\n",
      "   STYLE  PHOTO\n",
      "   VALUE  IMAGE\n",
      "   COLOR  BLACK-AND-WHITE\n",
      "   HEIGHT  200\n",
      "   WIDTH  200\n",
      "   SCREEN-X  200\n",
      "   SCREEN-Y  200\n",
      "   DISTANCE  1080\n",
      "   SIZE  111.939995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['IMAGE-LOCATION0-1']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actr.run(0.3)\n",
    "actr.buffer_chunk('visual')\n",
    "actr.buffer_chunk('visual-location')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
